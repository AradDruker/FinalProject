{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (491995020.py, line 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[7], line 30\u001b[1;36m\u001b[0m\n\u001b[1;33m    0.2 seconds\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "# Specify device as CPU\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Define the input tensor (batch_size=1, channels=1, height=5, width=5)\n",
    "input_tensor = torch.randn(1, 1, 5, 5).to(device)\n",
    "\n",
    "# Define a convolutional layer with a 5x5 kernel and move it to the CPU\n",
    "conv_layer = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=5, bias=False).to(device)\n",
    "\n",
    "# Warm-up (important to stabilize timings)\n",
    "for _ in range(10):\n",
    "    _ = conv_layer(input_tensor)\n",
    "\n",
    "# Run multiple iterations and measure time\n",
    "num_iterations = 1000\n",
    "start_time = time.time_ns()  # Start time in nanoseconds\n",
    "for _ in range(num_iterations):\n",
    "    output = conv_layer(input_tensor)\n",
    "end_time = time.time_ns()  # End time in nanoseconds\n",
    "\n",
    "# Calculate the elapsed time\n",
    "elapsed_time_ns = (end_time - start_time) / num_iterations  # Average time per iteration\n",
    "\n",
    "print(f\"Output:\\n{output}\")\n",
    "print(f\"Average time taken for convolution on CPU: {elapsed_time_ns:.2f} nanoseconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5x5 Kernel Centered on First Pixel:\n",
      "tensor([[  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0, 255, 255, 255],\n",
      "        [  0,   0, 255, 255, 255],\n",
      "        [  0,   0, 255, 255, 255]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Given 28x28 tensor (replace with your actual tensor)\n",
    "tensor_28x28 = torch.tensor([[255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
    "           255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
    "           255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
    "           255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
    "           211, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 136, 137,  54,\n",
    "           211, 138,  72, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 217,  59,  44,   7,\n",
    "            21,  11,   4,  11,  16,  16, 255, 255, 255, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255, 255, 255, 255, 170, 170,  59,   7,  15,\n",
    "            13,  53, 105,   2,   2,   8, 126, 210, 255, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255, 255, 255, 255, 152,  29,   0,  51, 161,\n",
    "           186, 186, 215, 125, 118,   8,  19,  57, 253, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255, 255, 255, 255, 181,  37,   0,  65, 176,\n",
    "           252, 255, 224, 221, 188,  13,   7,   7, 154, 248, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255, 255, 255, 235,  85,  17,  35,  35, 126,\n",
    "           255, 255, 255, 255, 252,  71,  10,  27, 180, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255, 255, 255, 255, 196,  84,   1,  40, 222,\n",
    "           254, 255, 255, 255, 250, 120,  26,  27, 123, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255, 255, 255, 255, 255,  84,  23,   7, 123,\n",
    "           199, 219, 251, 194,  67,  24,  14,  96,  96, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255, 255, 255, 255, 244, 138,  46,   1,   7,\n",
    "            37,  95, 193,  77,  67,   7,  37, 102, 255, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255, 255, 255, 235, 118, 164,  60,   1,   0,\n",
    "             0,  35,   6,   2,  16,  10,  85, 138, 255, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255, 183, 116,  33,   4,  38,   9,   3,   0,\n",
    "             0,   0,   0,  10,  53, 162, 253, 255, 255, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255, 114,  45,   4,  20,  15,  76,  28, 115,\n",
    "           151,   0,   0,   1, 121, 224, 253, 255, 255, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255,  86,  86,  22,   0,  20,  96, 196, 177, 225,\n",
    "           235,  73,  17,   0,  68, 173, 251, 255, 255, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 112,  21,   0,  99, 183, 252, 252, 244, 255,\n",
    "           255, 213, 143,   0,   9,  54, 255, 255, 255, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 186,  21,   0,  57, 210, 255, 255, 255, 255,\n",
    "           255, 230, 230,  52,   2,   9, 154, 255, 255, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 155,   9,   9,   0, 140, 200, 219, 255, 255,\n",
    "           255, 255, 167,  41,   0,  32, 222, 222, 255, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255,  62,  22,  12,  73, 114, 222, 213, 213,\n",
    "           255, 229, 124,  56,   0,  29, 170, 255, 255, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255, 220, 148,  12,   5,   1, 121, 101,  51,\n",
    "           134,  50,   1,   1,   1,  29, 155, 255, 255, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255, 255, 255,  32,  32,   9,   1,   4,   7,\n",
    "            12,   1,   1,  17,  89, 194, 255, 255, 255, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,  24,  50, 105,\n",
    "            16,  31,  16,  77, 186, 241, 255, 255, 255, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 105,\n",
    "           180, 236, 185, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
    "           255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
    "           255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
    "           255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255]])\n",
    "\n",
    "# Step 1: Add padding of 2 to the tensor\n",
    "padded_tensor = F.pad(tensor_28x28, pad=(2, 2, 2, 2), mode='constant', value=0)\n",
    "\n",
    "# Step 2: Extract the 5x5 kernel around the first pixel\n",
    "# The first pixel (0, 0) in the original tensor corresponds to (2, 2) in the padded tensor\n",
    "kernel_5x5 = padded_tensor[0:5, 0:5]  # Centered at (2, 2) in padded tensor\n",
    "\n",
    "# Print the result\n",
    "print(\"5x5 Kernel Centered on First Pixel:\")\n",
    "print(kernel_5x5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not working!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias quant (Quantized): 8547\n",
      "Output (With Bias): 8951\n",
      "Quantized Output (8-bit): 26\n",
      "M scale: 0.0029076067472083695\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Quantized input tensor (5x5 region in uint8)\n",
    "xq = torch.tensor([\n",
    "    [  0,   0,   0,   0, 0],\n",
    "    [ 0,   0,   0,   0, 0],\n",
    "    [   0,   0, 254, 254, 254],\n",
    "    [  0,   0,254, 254, 254],\n",
    "    [0,   0, 254, 254, 254],\n",
    "], dtype=torch.uint8)  # Shape: (5, 5)\n",
    "\n",
    "# Quantized weights (5x5 kernel in int8)\n",
    "wq = torch.tensor([\n",
    "    [-9,   48,   20,   54,  -16],\n",
    "    [ 74,   72,   17,   34,  -11],\n",
    "    [-15,   13,-3,  -18,   42],\n",
    "    [   -98,  -65,   13,  -37,    7],\n",
    "    [-84, -127,  -31,   17,25],\n",
    "], dtype=torch.int8)  # Shape: (5, 5)\n",
    "\n",
    "# Quantization parameters\n",
    "input_scale = 0.0078\n",
    "input_zero_point = 127\n",
    "\n",
    "output_scale = 0.0108  # Scale for input activations\n",
    "output_zero_point = 0  # Zero-point for input activations\n",
    "\n",
    "weight_scale = 0.004025917034596205  # Scale for weights\n",
    "weight_zero_point = 0  # Zero-point for weights\n",
    "\n",
    "bias_float = 0.2684  # Bias in floating-point\n",
    "effective_scale = input_scale * weight_scale # Scale for the output\n",
    "\n",
    "xq_signed = xq.to(torch.int16) - input_zero_point  # Shift to zero-centered\n",
    "\n",
    "z = torch.sum(xq_signed.to(torch.int8) * wq.to(torch.int8)).item()\n",
    "\n",
    "bias_q  = round(bias_float / effective_scale) # Bias in quantized form\n",
    "print(\"Bias quant (Quantized):\", bias_q)\n",
    "\n",
    "z_int = z + bias_q\n",
    "\n",
    "# Step 4: Quantize the result back to 8-bit (uint8)\n",
    "M = effective_scale / output_scale\n",
    "z_out = round(z_int * M - output_zero_point)  # Quantized output\n",
    "\n",
    "# Step 5: Clamp to the valid uint8 range [0, 255]s\n",
    "output_quantized = max(0, min(255, z_out))\n",
    "\n",
    "# Display results\n",
    "print(\"Output (With Bias):\", z_int)\n",
    "print(\"Quantized Output (8-bit):\", output_quantized)\n",
    "print(\"M scale:\", M)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create coe files for conv1,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_coe_file_hex_with_padding(numbers1, numbers2, output_file):\n",
    "    \"\"\"\n",
    "    Generate a COE file with two concatenated hexadecimal numbers.\n",
    "    Each list is processed as follows:\n",
    "      - All but the last number are represented as 8-bit signed binary values (2's complement).\n",
    "      - The last number is represented as a 32-bit signed binary value (2's complement).\n",
    "\n",
    "    The output file will have:\n",
    "      memory_initialization_radix=16;\n",
    "      memory_initialization_vector=\n",
    "      <hex_for_first_array> <hex_for_second_array>;\n",
    "\n",
    "    Parameters:\n",
    "        numbers1 (list of int): The first input array of signed integers.\n",
    "        numbers2 (list of int): The second input array of signed integers.\n",
    "        output_file (str): The name of the output COE file.\n",
    "    \"\"\"\n",
    "    def process_numbers(numbers):\n",
    "        if not isinstance(numbers, list) or not all(isinstance(n, int) for n in numbers):\n",
    "            raise ValueError(\"Each input must be a list of signed integers.\")\n",
    "\n",
    "        if len(numbers) < 2:\n",
    "            raise ValueError(\"Each input list must contain at least two numbers.\")\n",
    "\n",
    "        binary_values = []\n",
    "\n",
    "        # Process all numbers except the last as 8-bit signed binary\n",
    "        for num in numbers[:-1]:\n",
    "            if num < -128 or num > 127:\n",
    "                raise ValueError(f\"Number {num} exceeds the range of an 8-bit signed integer (-128 to 127).\")\n",
    "            binary = f\"{num & 0xFF:08b}\"  # 8-bit 2's complement\n",
    "            binary_values.append(binary)\n",
    "\n",
    "        # Process the last number as 32-bit signed binary\n",
    "        last_num = numbers[-1]\n",
    "        if last_num < -2**31 or last_num > 2**31 - 1:\n",
    "            raise ValueError(f\"Last number {last_num} exceeds the range of a 32-bit signed integer.\")\n",
    "        last_binary = f\"{last_num & 0xFFFFFFFF:032b}\"\n",
    "\n",
    "        # Combine all binary values\n",
    "        concatenated_binary = \"\".join(binary_values) + last_binary\n",
    "\n",
    "        # Convert binary string to hexadecimal\n",
    "        concatenated_hex = f\"{int(concatenated_binary, 2):X}\".zfill(len(concatenated_binary) // 4)\n",
    "        return concatenated_hex\n",
    "\n",
    "    # Process both arrays\n",
    "    hex1 = process_numbers(numbers1)\n",
    "    hex2 = process_numbers(numbers2)\n",
    "\n",
    "    # Write to COE file\n",
    "    with open(output_file, \"w\") as coe_file:\n",
    "        coe_file.write(\"memory_initialization_radix=16;\\n\")\n",
    "        coe_file.write(\"memory_initialization_vector=\\n\")\n",
    "        coe_file.write(f\"{hex1} {hex2};\\n\")\n",
    "\n",
    "    return hex1, hex2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import csv\n",
    "import torch.quantization as quant\n",
    "import warnings\n",
    "from PIL import Image\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\".*weights_only=False.*\",  # Match the specific warning message\n",
    "    category=FutureWarning\n",
    ")\n",
    "\n",
    "def save_tensor_to_csv_as_matrix(tensor, filename):\n",
    "    \"\"\"\n",
    "    Save a PyTorch tensor to a CSV file, formatted as matrices for each channel.\n",
    "    Each matrix represents the spatial dimensions (Height x Width) for a channel,\n",
    "    with rows separated by commas.\n",
    "\n",
    "    Parameters:\n",
    "        tensor (torch.Tensor): The tensor to save (e.g., Batch x Channels x Height x Width).\n",
    "        filename (str): The name of the CSV file to create.\n",
    "    \"\"\"\n",
    "    # Handle quantized tensors\n",
    "    if tensor.is_quantized:\n",
    "        tensor = tensor.int_repr()  # Extract the integer representation\n",
    "\n",
    "    # Convert the tensor to a NumPy array\n",
    "    numpy_array = tensor.detach().cpu().numpy()\n",
    "\n",
    "    # Ensure the tensor has 4 dimensions: [Batch, Channels, Height, Width]\n",
    "    if len(numpy_array.shape) != 4:\n",
    "        raise ValueError(\"Tensor must have 4 dimensions (Batch x Channels x Height x Width).\")\n",
    "\n",
    "    batch_size, num_channels, height, width = numpy_array.shape\n",
    "\n",
    "    if batch_size != 1:\n",
    "        raise ValueError(\"Only batch size of 1 is supported.\")\n",
    "\n",
    "    # Open the file and write matrices for each channel\n",
    "    with open(filename, mode=\"w\") as file:\n",
    "        for channel in range(num_channels):\n",
    "            file.write(f\"Channel {channel + 1}:\\n\")  # Channel header\n",
    "            for i, row in enumerate(numpy_array[0, channel]):  # Iterate over height (rows)\n",
    "                file.write(f\"{list(row)}\")  # Format each row as a list\n",
    "                if i < height - 1:\n",
    "                    file.write(\",\\n\")  # Add a comma between rows\n",
    "                else:\n",
    "                    file.write(\"\\n\")  # No comma after the last row\n",
    "            file.write(\"\\n\")  # Add a newline between channels\n",
    "\n",
    "    print(f\"Tensor saved to {filename} in matrix format.\")\n",
    "\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, num_classes=33):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.quant = torch.ao.quantization.QuantStub()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(84, num_classes)\n",
    "        self.dequant = torch.ao.quantization.DeQuantStub()\n",
    "        self.print_activation = False  # Add a flag to control printing\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        if self.print_activation:\n",
    "            # print(\"quant output shape:\", x)\n",
    "            # save_tensor_to_csv_as_matrix(x, \"image_output.csv\")\n",
    "            pass\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        if self.print_activation:\n",
    "            # print(\"Conv1 output shape:\", x.int_repr())\n",
    "            # save_tensor_to_csv_as_matrix(x, \"relu1_output.csv\")\n",
    "            pass\n",
    "        x = self.pool1(x)\n",
    "        if self.print_activation:\n",
    "            # print(\"Conv1 output shape:\", x.int_repr())\n",
    "            # save_tensor_to_csv_as_matrix(x, \"pool1_output.csv\")\n",
    "            pass\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = x.reshape(-1, 16 * 5 * 5)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "def load_quantized_model_and_labels(model_path, num_classes, device):\n",
    "    # Re-create the float model\n",
    "    model = LeNet5(num_classes=num_classes)\n",
    "    model.eval()\n",
    "\n",
    "    # Set the same QAT configuration as before\n",
    "    custom_qconfig = quant.QConfig(\n",
    "        activation=quant.FakeQuantize.with_args(observer=quant.MinMaxObserver, quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine),\n",
    "        weight=quant.FakeQuantize.with_args(observer=quant.MinMaxObserver, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric)\n",
    "    )\n",
    "\n",
    "    # Apply the custom QConfig to the model\n",
    "    model.qconfig = custom_qconfig\n",
    "\n",
    "    # Fuse modules just like during training\n",
    "    model_fused = torch.ao.quantization.fuse_modules(\n",
    "        model,\n",
    "        [['conv1', 'relu1'], ['conv2', 'relu2'], ['fc1', 'relu3'], ['fc2', 'relu4']]\n",
    "    )\n",
    "\n",
    "    # Prepare for QAT (simulates the same steps taken during training)\n",
    "    model_prepared = torch.ao.quantization.prepare_qat(model_fused.train(), inplace=True)\n",
    "\n",
    "    # Switch to eval mode before feeding dummy data\n",
    "    model_prepared.eval()\n",
    "\n",
    "    dummy_input = torch.randn(1, 1, 28, 28).to(device)\n",
    "    with torch.no_grad():\n",
    "        _ = model_prepared(dummy_input)  # Populates the observers\n",
    "\n",
    "    # Now convert the model to quantized form after observers have data\n",
    "    model_int8 = torch.ao.quantization.convert(model_prepared.eval())\n",
    "\n",
    "    # Load the saved quantized model weights and labels\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model_int8.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    # Check if 'labels_mapping' exists in the checkpoint\n",
    "    labels_mapping = checkpoint.get('labels_mapping', None)  # Default to None if key doesn't exist\n",
    "\n",
    "    # Move to device and set to eval mode\n",
    "    model_int8.to(device)\n",
    "    model_int8.eval()\n",
    "\n",
    "    if labels_mapping is not None:\n",
    "        return model_int8, labels_mapping\n",
    "    else:\n",
    "        return model_int8, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Channels in Layer 1 is: 6\n",
      "('2416DD05F31B0F26F2AB0B420F98E6FE24E1CB070108DDCC3400001663', 'F1FCDE0FF3F1191E0E100810F30714E81A23171E14F50A25E5FFFFF87C') Channel: 1,2\n",
      "('E71F354D500F1B48342F3139F9F206E2FBBDA6B7B2FC9B9BEF000015C1', 'EBA3818FB0E8D20AF0DC2F34152FDE11344833F9270C2D1F3000001BB0') Channel: 3,4\n",
      "('EB1E131804B82401180BD8B3F443E7ACD43840E8AA0EE7F51800002C47', 'C2D3FE234B09A9B5EC4837209D9A1A0325239ABC1E2D491ACA00003590') Channel: 5,6\n",
      "Num of Channels in Layer 2 is: 16\n",
      "('04142732151018222C1F1E1A2226032310181EFD0710042108FFFFFE8D', '03F4F9CFE706D1D0EDE5D2B1EF07EAE3EA1111011F12030F05FFFFFD1A') Channel: 1,2\n",
      "('B29ECEC0B7D5CCCABCA10903F0D29B0E1F0903B91B162D16AC000003FF', '0404C3AC03160BF2BDFC0A1AF1D301F11BF8DD16F01EF5E014FFFFFE92') Channel: 3,4\n",
      "('061CE8D0F719FADFEDFD000610100C0030160207FCFADEE106FFFFFF8B', 'E8F7000CFFECE0130722FB2226FB1BCEFDD5EDE2F6EFD4E0D5000000A7') Channel: 5,6\n",
      "('E819402A10E8F42A1707E21227210FB3122C2B0DE52A312E0EFFFFFB74', 'F2FC052002EE0200040AF80AFC1518E5E6F0F612EB0603EF19FFFFFC63') Channel: 7,8\n",
      "('BCE205C203A2BB81B8E7A7AEA8EFF1EEE5FA1311051204060A000004B7', '08010E141408E9E9E3E7F0CEB5C3F700F7CB0113042415FAFAFFFFFF2E') Channel: 9,10\n",
      "('00F904FE02050305FD0601F30408FDF9FB0203FAFCF9FCFAFCFFFFFD72', 'D92134FD00BAEBFEF4EFA0A9E5DEEAD8D1DFFAE70B07FFF5E500000384') Channel: 11,12\n",
      "('8FC2AFACAABBC6D09A92D80F0DF9E5E516EAFEADE1EFE3D8C000000390', '1A02FBFED6170615150807070E1621FB09FC0E070EFAEE07F7FFFFFC83') Channel: 13,14\n",
      "('EDE4FDEBDDD70E15E6D8C90402DCCCC3F9EFBCB2D0E3F9CCC7000002B0', '09CBDD2531EE0306310F02352D25110B22140FEE0DF81207F2FFFFFE1D') Channel: 15,16\n"
     ]
    }
   ],
   "source": [
    "def create_coe_files_convs(path, num_classes):\n",
    "    # Example usage (ensure you have defined test_loader and dataset.int_to_char):\n",
    "    model_path = path  # Path to your saved quantized model\n",
    "    device = torch.device(\"cpu\")\n",
    "      # Update with your number of classes\n",
    "\n",
    "    # Input name, Activation Name\n",
    "    names_list = [['quant', 'conv1'],['conv1','conv2']]\n",
    "    for layer in range(len(names_list)):\n",
    "        # Model parameters\n",
    "        model_int8, labels  = load_quantized_model_and_labels(model_path, num_classes, device)\n",
    "        input_scale = model_int8.state_dict()[f'{names_list[layer][0]}.scale']\n",
    "        input_zero_point = model_int8.state_dict()[f'{names_list[layer][0]}.zero_point']\n",
    "        weight_scale = model_int8.state_dict()[f'{names_list[layer][1]}.weight'].q_scale()\n",
    "        weight_zero_point = model_int8.state_dict()[f'{names_list[layer][1]}.weight'].q_zero_point()\n",
    "        effective_scale = input_scale * weight_scale # Scale for the output\n",
    "\n",
    "        # Creating weights and Biases Lists\n",
    "        active_weights = model_int8.state_dict()[f'{names_list[layer][1]}.weight'].int_repr()\n",
    "        active_biases = model_int8.state_dict()[f'{names_list[layer][1]}.bias']\n",
    "        active_biases_list = active_biases.tolist()\n",
    "        q_biases = []\n",
    "        for i in range(len(active_biases_list)):\n",
    "            q_value = float(active_biases_list[i]) / effective_scale\n",
    "            q_biases.append(round(float(q_value)))\n",
    "\n",
    "        parameters_list = []\n",
    "        # Print weights for each channel with separation\n",
    "        num_channels = active_weights.shape[0]  # Number of channels in conv1\n",
    "        for channel in range(num_channels):\n",
    "            # Convert the first 25 weights to a list\n",
    "            channel_list = active_weights[channel].reshape(-1)[:25].tolist()\n",
    "            channel_list.append(q_biases[channel])  # Append the bias of the current channel\n",
    "            parameters_list.append(channel_list)\n",
    "\n",
    "        print(f'Num of Channels in Layer {layer+1} is: {len(parameters_list)}')\n",
    "\n",
    "        for i in range(0,len(parameters_list)-1,2):\n",
    "            print(f'{generate_coe_file_hex_with_padding(parameters_list[i], parameters_list[i+1], f'Layer{layer+1}_Channel{i+1}_{i+2}.coe')} Channel: {i+1},{i+2}')\n",
    "\n",
    "create_coe_files_convs('lenetV5.pth', 33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
